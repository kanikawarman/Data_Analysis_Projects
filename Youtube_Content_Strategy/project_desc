Project: YouTube Content Strategy Optimization
ðŸŽ¯ Objective

To design a framework that helps creators and analysts understand what types of content perform well, why they perform well, and how to replicate success across videos.

ðŸ”§ What I Did

Data Acquisition & Preparation

Used the Kaggle dataset (videos-stats.csv, comments.csv).

Cleaned raw data (handled -1 values for hidden likes/comments, parsed dates, ensured numeric typing).

Standardized schema for integration across video-level and comment-level datasets.

Feature Engineering & Metrics

Performance Percentiles: Computed 7-day, 30-day, and all-time view percentiles using rolling calendar windows.

Engagement Rates: Calculated like_rate (likes/views) and comment_rate (comments/views).

Cohort Analysis: Grouped videos by publish month, estimated cumulative views/day, and plotted growth curves.

Comment Controversy Score: Combined variance of sentiments, entropy of sentiment distribution, and variance in comment likes â†’ normalized to a 0â€“1 score.

Keyword Aggregation: Rolled up controversy scores at keyword level to identify polarizing topics.

Analysis & Insights

Identified consistently high performers (â‰¥80th percentile across all timeframes).

Correlated sentiment signals with video metrics to measure engagement quality.

Generated keyword-level controversy rankings and visualizations.

Outputs & Deliverables

CSV reports: enriched video metrics, cohort curves, keyword controversy, correlation matrices.

Plots: cohort trajectories, keyword controversy bar chart, correlation heatmap, most controversial videos, consistent high performers.

Packaged as a modular Python project with config, pipeline, and CLI for reproducibility.

ðŸ› ï¸ How I Did It

Tech Stack: Python (Pandas, NumPy, Matplotlib, Seaborn), YAML for config.

Architecture: Modular project with components:

percentiles.py â†’ rolling percentiles

cohorts.py â†’ cohort curves

controversy.py â†’ controversy score

analytics.py â†’ correlations, high performers, keyword analysis

visualize.py â†’ plots

Pipeline: Automated end-to-end run via main.py CLI, outputs CSVs & plots.

Design Choice: Since dataset provided only snapshot totals (not daily timeseries), cohort analysis estimated cumulative growth by normalizing views/day.

ðŸ’¡ Why It Matters
---
ðŸŽ¤ Interview Q&A
Technical Questions

Q1. How did you calculate the 7-day and 30-day percentiles without daily views data?

Answer:
The dataset only provides total views per video. To approximate short-term performance, I built calendar-based peer groups: for each video, I compared its total views at the snapshot against all other videos published in the previous 7 or 30 calendar days. The percentile is its relative standing within that peer set.

Follow-ups:

How would results differ if daily view counts were available?
â†’ With daily counts, I could compute true growth curves (cumulative views at day 7, day 30) instead of approximations.

What biases could this introduce?
â†’ Videos with longer lifetimes might look better in 30-day comparisons; fresh videos may seem weaker without daily breakdowns.

Q2. Why did you choose variance & entropy for the controversy score? Could you have used other measures?

Answer:

Variance of sentiment scores captures how spread-out opinions are (e.g., mix of negatives and positives).

Entropy measures disorder: higher when negative, neutral, and positive are equally likely â†’ more polarized discussion.

Variance in comment likes adds another dimension: if some comments get extreme attention (positive or negative), it signals heated debate.
These were combined and normalized to form a balanced score.

Follow-ups:

What alternatives could you use?
â†’ Gini index for inequality in likes, KL divergence to compare sentiment distributions vs. a â€œneutral baseline,â€ or even topic modeling for semantic controversy.

Why average them instead of weighting?
â†’ Averaging avoids overweighting one signal. A weighted scheme could be tuned if we validated against external â€œcontroversyâ€ ground truth.

Q3. How did you handle missing or hidden likes/comments?

Answer:
In videos-stats.csv, -1 indicated hidden counts. I converted these to NaN so they donâ€™t distort averages or ratios. For rates (likes/views, comments/views), I only computed them if both numerator and denominator were valid.

Follow-ups:

Whatâ€™s the risk of excluding too many rows?
â†’ Reduced sample size and potential bias if creators who hide metrics systematically differ (e.g., low engagement).

How could you mitigate?
â†’ Use imputation strategies, median substitution, or treat â€œhiddenâ€ as a feature itself (signal of creator behavior).

Q4. What are the limitations of the cohort analysis given the dataset?

Answer:
The dataset only has total views at snapshot time. To simulate growth, I estimated views/day = total views Ã· age (days since publish). Then projected cumulative growth curves for each cohort. This assumes stable growth, which is unrealistic but useful for comparing relative performance.

Follow-ups:

How would you improve it?
â†’ With daily view data, compute true cumulative curves and retention metrics (e.g., median views at day 30).

Could you still use proxy features?
â†’ Yes, engagement velocity proxies: likes/day, comments/day, or ratios (likes/views).

Q5. Explain correlation results â€” did controversy positively or negatively affect views?

Answer:
In my analysis, controversy score showed a moderate positive correlation with views â€” suggesting polarizing videos often get more exposure. However, controversy correlated negatively with like_rate (audience approval), implying divisive content attracts attention but may reduce approval.

Follow-ups:

Could correlation imply causation?
â†’ Not necessarily; external promotion might drive both views and controversial comments.

How would you test causality?
â†’ Use time-series data or A/B test variations of controversial vs. non-controversial content.

Conceptual Questions

Q6. Whatâ€™s the difference between high-performing and consistently high-performing content?

Answer:

High-performing = videos in top percentiles in one timeframe (e.g., viral in first 7 days).

Consistently high-performing = videos in â‰¥80th percentile in 7-day, 30-day, and all-time â€” meaning they sustain performance across short and long horizons.

Follow-ups:

Why is consistency important?
â†’ It signals evergreen appeal, not just short-term virality.

Q7. How would you extend this framework to predict future video performance?

Answer:
Add predictive modeling:

Features: early engagement velocity (views/day in first week), sentiment distribution, keyword/topic, channel history.

Models: regression or gradient boosting to predict long-term views.

Follow-ups:

What metric would you predict?
â†’ Views at day 30 or day 90.

How would you evaluate?
â†’ MAE/RMSE for regression, rank correlation for relative performance prediction.

Q8. How do sentiment distributions affect different engagement metrics?

Answer:

Higher positive sentiment fraction â†’ correlated with like_rate.

Higher negative sentiment fraction â†’ correlated with comment_rate (people argue more).

Mixed entropy â†’ correlated with views (debate attracts clicks).

Follow-ups:

How would you prove this robustly?
â†’ Stratify analysis by keyword/category, run statistical tests (t-test, ANOVA).

Q9. What real-world business applications do you see?

Answer:

For creators: Identify what to double down on.

For brands: Benchmark campaign content and avoid negative-PR risks.

For platforms: Recommend balanced content strategies.

Follow-ups:

How would you turn this into a product?
â†’ Build a dashboard (e.g., Streamlit/PowerBI) with cohort plots, controversy heatmaps, and alerts for viral-but-divisive content.

Q10. How would you productize this as a dashboard?

Answer:

Backend: automated data pipeline (ETL from YouTube API).

Frontend: dashboard showing cohorts, high performers, controversy, keyword rankings.

Alerts: notify when controversy > threshold or engagement velocity > baseline.

Follow-ups:

Which stack would you use?
â†’ Streamlit for MVP; Airflow + dbt for pipelines; Tableau/PowerBI for enterprise.

For Creators: Understand which content consistently wins, how controversy drives discussion, and when to post for long-term traction.

For Analysts/Brands: Build repeatable benchmarks for content strategy optimization.

For Recruiters: Demonstrates end-to-end data science workflow â€” data cleaning, feature engineering, analytics, visualization, reproducible code structure.
